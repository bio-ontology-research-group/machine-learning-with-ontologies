{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'elembeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d797e51bd6d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m from elembeddings.elembedding import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     ELModel, load_data, load_valid_data, Generator, MyModelCheckpoint)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'elembeddings'"
     ]
    }
   ],
   "source": [
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import rankdata\n",
    "import os\n",
    "\n",
    "from elembeddings.elembedding import (\n",
    "    ELModel, load_data, load_valid_data, Generator, MyModelCheckpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 256\n",
    "embedding_size = 50\n",
    "margin = -0.1\n",
    "reg_norm = 1\n",
    "learning_rate = 1e-3\n",
    "epochs = 200\n",
    "org_id = '9606'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf1 85119\n",
      "nf2 12090\n",
      "nf3 749275\n",
      "nf4 12088\n",
      "disjoint 30\n",
      "nf3_neg 1076572\n",
      "top 1\n"
     ]
    }
   ],
   "source": [
    "# Load training data in (h, l, t) triples\n",
    "# classes and relations are entity to id mappings\n",
    "train_data, classes, relations = load_data(f'data/train/{org_id}.classes-normalized.owl')\n",
    "valid_data = load_valid_data(f'data/valid/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "for key, value in train_data.items():\n",
    "    print(f'{key} {len(value)}')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of classes 61810\n",
      "Total number of relations 10\n"
     ]
    }
   ],
   "source": [
    "# Filter out protein classes\n",
    "proteins = {}\n",
    "for k, v in classes.items():\n",
    "    if not k.startswith('<http://purl.obolibrary.org/obo/GO_'):\n",
    "        proteins[k] = v\n",
    "\n",
    "# Prepare data for training the model\n",
    "nb_classes = len(classes)\n",
    "nb_relations = len(relations)\n",
    "nb_train_data = 0\n",
    "for key, val in train_data.items():\n",
    "    nb_train_data = max(len(val), nb_train_data)\n",
    "train_steps = int(math.ceil(nb_train_data / (1.0 * batch_size)))\n",
    "train_generator = Generator(train_data, batch_size, steps=train_steps)\n",
    "\n",
    "# id to entity maps\n",
    "cls_dict = {v: k for k, v in classes.items()}\n",
    "rel_dict = {v: k for k, v in relations.items()}\n",
    "\n",
    "cls_list = []\n",
    "rel_list = []\n",
    "for i in range(nb_classes):\n",
    "    cls_list.append(cls_dict[i])\n",
    "for i in range(nb_relations):\n",
    "    rel_list.append(rel_dict[i])\n",
    "\n",
    "        \n",
    "print('Total number of classes', nb_classes)\n",
    "print('Total number of relations', nb_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ELEmbeddings Model and Train\n",
    "\n",
    "Embeddings are saved depending on mean rank evaluation on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kulmanm/KAUST/CBRC/ontology-tutorial/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1190/1191 [============================>.] - ETA: 0s - loss: 53.7760\n",
      " Saving embeddings 1 1742.7905787526427\n",
      "\n",
      "1191/1191 [==============================] - 84s 71ms/step - loss: 53.7475\n",
      "Epoch 2/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 14.9765\n",
      "Epoch 3/200\n",
      "1189/1191 [============================>.] - ETA: 0s - loss: 7.9839\n",
      " Saving embeddings 3 1742.0270877378437\n",
      "\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 7.9803\n",
      "Epoch 4/200\n",
      "1189/1191 [============================>.] - ETA: 0s - loss: 4.0507\n",
      " Saving embeddings 4 1735.8652616279069\n",
      "\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 4.0487\n",
      "Epoch 5/200\n",
      "1189/1191 [============================>.] - ETA: 0s - loss: 1.6379\n",
      " Saving embeddings 5 1698.898506871036\n",
      "\n",
      "1191/1191 [==============================] - 78s 65ms/step - loss: 1.6366\n",
      "Epoch 6/200\n",
      "1189/1191 [============================>.] - ETA: 0s - loss: 0.4325\n",
      " Saving embeddings 6 1469.570665961945\n",
      "\n",
      "1191/1191 [==============================] - 78s 66ms/step - loss: 0.4321\n",
      "Epoch 7/200\n",
      "1188/1191 [============================>.] - ETA: 0s - loss: 0.1438\n",
      " Saving embeddings 7 1070.4563028541227\n",
      "\n",
      "1191/1191 [==============================] - 77s 64ms/step - loss: 0.1437\n",
      "Epoch 8/200\n",
      "1189/1191 [============================>.] - ETA: 0s - loss: 0.0962\n",
      " Saving embeddings 8 899.4200581395348\n",
      "\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0962\n",
      "Epoch 9/200\n",
      "1189/1191 [============================>.] - ETA: 0s - loss: 0.0815\n",
      " Saving embeddings 9 781.0330866807611\n",
      "\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0815\n",
      "Epoch 10/200\n",
      "1188/1191 [============================>.] - ETA: 0s - loss: 0.0754\n",
      " Saving embeddings 10 738.529717230444\n",
      "\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0755\n",
      "Epoch 11/200\n",
      "1188/1191 [============================>.] - ETA: 0s - loss: 0.0694\n",
      " Saving embeddings 11 652.0751057082452\n",
      "\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0694\n",
      "Epoch 12/200\n",
      "1189/1191 [============================>.] - ETA: 0s - loss: 0.0652\n",
      " Saving embeddings 12 571.3252246300211\n",
      "\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0652\n",
      "Epoch 13/200\n",
      "1190/1191 [============================>.] - ETA: 0s - loss: 0.0607\n",
      " Saving embeddings 13 548.2884381606765\n",
      "\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0607\n",
      "Epoch 14/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0553\n",
      "Epoch 15/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0532\n",
      "Epoch 16/200\n",
      "1191/1191 [==============================] - 79s 66ms/step - loss: 0.0491\n",
      "Epoch 17/200\n",
      "1190/1191 [============================>.] - ETA: 0s - loss: 0.0468\n",
      " Saving embeddings 17 520.8179968287526\n",
      "\n",
      "1191/1191 [==============================] - 77s 64ms/step - loss: 0.0468\n",
      "Epoch 18/200\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0454\n",
      "Epoch 19/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0435\n",
      "Epoch 20/200\n",
      "1191/1191 [==============================] - 76s 63ms/step - loss: 0.0418\n",
      "Epoch 21/200\n",
      "1191/1191 [==============================] - 76s 63ms/step - loss: 0.0404\n",
      "Epoch 22/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0395\n",
      "Epoch 23/200\n",
      "1190/1191 [============================>.] - ETA: 0s - loss: 0.0384\n",
      " Saving embeddings 23 514.8735332980973\n",
      "\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0383\n",
      "Epoch 24/200\n",
      "1189/1191 [============================>.] - ETA: 0s - loss: 0.0377\n",
      " Saving embeddings 24 485.69746300211415\n",
      "\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0377\n",
      "Epoch 25/200\n",
      "1191/1191 [==============================] - 79s 66ms/step - loss: 0.0367\n",
      "Epoch 26/200\n",
      "1191/1191 [==============================] - 74s 63ms/step - loss: 0.0350\n",
      "Epoch 27/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0354\n",
      "Epoch 28/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0342\n",
      "Epoch 29/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0341\n",
      "Epoch 30/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0328\n",
      "Epoch 31/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0330\n",
      "Epoch 32/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0321\n",
      "Epoch 33/200\n",
      "1191/1191 [==============================] - 73s 62ms/step - loss: 0.0328\n",
      "Epoch 34/200\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0315\n",
      "Epoch 35/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0312\n",
      "Epoch 36/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0305\n",
      "Epoch 37/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0307\n",
      "Epoch 38/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0307\n",
      "Epoch 39/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0301\n",
      "Epoch 40/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0300\n",
      "Epoch 41/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0294\n",
      "Epoch 42/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0292\n",
      "Epoch 43/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0289\n",
      "Epoch 44/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0294\n",
      "Epoch 45/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0294\n",
      "Epoch 46/200\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.0290\n",
      "Epoch 47/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0276\n",
      "Epoch 48/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0285\n",
      "Epoch 49/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0288\n",
      "Epoch 50/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0278\n",
      "Epoch 51/200\n",
      "1191/1191 [==============================] - 78s 65ms/step - loss: 0.0286\n",
      "Epoch 52/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0276\n",
      "Epoch 53/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0276\n",
      "Epoch 54/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0268\n",
      "Epoch 55/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0272\n",
      "Epoch 56/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0272\n",
      "Epoch 57/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0269\n",
      "Epoch 58/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0278\n",
      "Epoch 59/200\n",
      "1191/1191 [==============================] - 76s 63ms/step - loss: 0.0264\n",
      "Epoch 60/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0273\n",
      "Epoch 61/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0269\n",
      "Epoch 62/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0272\n",
      "Epoch 63/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0265\n",
      "Epoch 64/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0272\n",
      "Epoch 65/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0264\n",
      "Epoch 66/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0261\n",
      "Epoch 67/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0260\n",
      "Epoch 68/200\n",
      "1191/1191 [==============================] - 79s 67ms/step - loss: 0.0263\n",
      "Epoch 69/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0263\n",
      "Epoch 70/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0258\n",
      "Epoch 71/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0262\n",
      "Epoch 72/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0258\n",
      "Epoch 73/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0263\n",
      "Epoch 74/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0254\n",
      "Epoch 75/200\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0258\n",
      "Epoch 76/200\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0253\n",
      "Epoch 77/200\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0246\n",
      "Epoch 78/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0253\n",
      "Epoch 79/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0260\n",
      "Epoch 80/200\n",
      "1191/1191 [==============================] - 76s 63ms/step - loss: 0.0246\n",
      "Epoch 81/200\n",
      "1191/1191 [==============================] - 76s 63ms/step - loss: 0.0250\n",
      "Epoch 82/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0247\n",
      "Epoch 83/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0252\n",
      "Epoch 84/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0243\n",
      "Epoch 85/200\n",
      "1191/1191 [==============================] - 81s 68ms/step - loss: 0.0242\n",
      "Epoch 86/200\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0245\n",
      "Epoch 87/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0238\n",
      "Epoch 88/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0240\n",
      "Epoch 89/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0242\n",
      "Epoch 90/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0248\n",
      "Epoch 91/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0250\n",
      "Epoch 92/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0244\n",
      "Epoch 93/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0242\n",
      "Epoch 94/200\n",
      "1191/1191 [==============================] - 81s 68ms/step - loss: 0.0244\n",
      "Epoch 95/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0249\n",
      "Epoch 96/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0242\n",
      "Epoch 97/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0233\n",
      "Epoch 98/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0243\n",
      "Epoch 99/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0245\n",
      "Epoch 100/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0239\n",
      "Epoch 101/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0239\n",
      "Epoch 102/200\n",
      "1191/1191 [==============================] - 77s 64ms/step - loss: 0.0234\n",
      "Epoch 103/200\n",
      "1191/1191 [==============================] - 79s 66ms/step - loss: 0.0241\n",
      "Epoch 104/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0241\n",
      "Epoch 105/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0239\n",
      "Epoch 106/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0237\n",
      "Epoch 107/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0230\n",
      "Epoch 108/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0231\n",
      "Epoch 109/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0237\n",
      "Epoch 110/200\n",
      "1191/1191 [==============================] - 73s 62ms/step - loss: 0.0228\n",
      "Epoch 111/200\n",
      "1191/1191 [==============================] - 78s 65ms/step - loss: 0.0233\n",
      "Epoch 112/200\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.0232\n",
      "Epoch 113/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0229\n",
      "Epoch 114/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0225\n",
      "Epoch 115/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0232\n",
      "Epoch 116/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0229\n",
      "Epoch 117/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0229\n",
      "Epoch 118/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0231\n",
      "Epoch 119/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0227\n",
      "Epoch 120/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0234\n",
      "Epoch 121/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0222\n",
      "Epoch 122/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0220\n",
      "Epoch 123/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0236\n",
      "Epoch 124/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0224\n",
      "Epoch 125/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0222\n",
      "Epoch 126/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0225\n",
      "Epoch 127/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0222\n",
      "Epoch 128/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0221\n",
      "Epoch 129/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0224\n",
      "Epoch 130/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0223\n",
      "Epoch 131/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0222\n",
      "Epoch 132/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0224\n",
      "Epoch 133/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0210\n",
      "Epoch 134/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0224\n",
      "Epoch 135/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0218\n",
      "Epoch 136/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0222\n",
      "Epoch 137/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0221\n",
      "Epoch 138/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0216\n",
      "Epoch 139/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0226\n",
      "Epoch 140/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0220\n",
      "Epoch 141/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0216\n",
      "Epoch 142/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0216\n",
      "Epoch 143/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0219\n",
      "Epoch 144/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0216\n",
      "Epoch 145/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0212\n",
      "Epoch 146/200\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0222\n",
      "Epoch 147/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0210\n",
      "Epoch 148/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0218\n",
      "Epoch 149/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0207\n",
      "Epoch 150/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0206\n",
      "Epoch 151/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0214\n",
      "Epoch 152/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0209\n",
      "Epoch 153/200\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.0205\n",
      "Epoch 154/200\n",
      "1191/1191 [==============================] - 76s 64ms/step - loss: 0.0198\n",
      "Epoch 155/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0216\n",
      "Epoch 156/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0212\n",
      "Epoch 157/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0204\n",
      "Epoch 158/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0213\n",
      "Epoch 159/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0211\n",
      "Epoch 160/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0224\n",
      "Epoch 161/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0209\n",
      "Epoch 162/200\n",
      "1191/1191 [==============================] - 77s 64ms/step - loss: 0.0213\n",
      "Epoch 163/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0205\n",
      "Epoch 164/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0204\n",
      "Epoch 165/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0201\n",
      "Epoch 166/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0203\n",
      "Epoch 167/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0200\n",
      "Epoch 168/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0207\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0198\n",
      "Epoch 170/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0209\n",
      "Epoch 171/200\n",
      "1191/1191 [==============================] - 75s 63ms/step - loss: 0.0208\n",
      "Epoch 172/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0200\n",
      "Epoch 173/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0208\n",
      "Epoch 174/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0201\n",
      "Epoch 175/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0195\n",
      "Epoch 176/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0208\n",
      "Epoch 177/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0202\n",
      "Epoch 178/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0199\n",
      "Epoch 179/200\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0199\n",
      "Epoch 180/200\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0198\n",
      "Epoch 181/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0195\n",
      "Epoch 182/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0204\n",
      "Epoch 183/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0201\n",
      "Epoch 184/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0204\n",
      "Epoch 185/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0201\n",
      "Epoch 186/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0204\n",
      "Epoch 187/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0202\n",
      "Epoch 188/200\n",
      "1191/1191 [==============================] - 74s 62ms/step - loss: 0.0194\n",
      "Epoch 189/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0193\n",
      "Epoch 190/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0197\n",
      "Epoch 191/200\n",
      "1191/1191 [==============================] - 71s 59ms/step - loss: 0.0193\n",
      "Epoch 192/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0192\n",
      "Epoch 193/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0197\n",
      "Epoch 194/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0196\n",
      "Epoch 195/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0194\n",
      "Epoch 196/200\n",
      "1191/1191 [==============================] - 77s 65ms/step - loss: 0.0193\n",
      "Epoch 197/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0195\n",
      "Epoch 198/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0202\n",
      "Epoch 199/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0193\n",
      "Epoch 200/200\n",
      "1191/1191 [==============================] - 71s 60ms/step - loss: 0.0184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad881a7ba8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input layers for each loss type\n",
    "nf1 = Input(shape=(2,), dtype=np.int32)\n",
    "nf2 = Input(shape=(3,), dtype=np.int32)\n",
    "nf3 = Input(shape=(3,), dtype=np.int32)\n",
    "nf4 = Input(shape=(3,), dtype=np.int32)\n",
    "dis = Input(shape=(3,), dtype=np.int32)\n",
    "top = Input(shape=(1,), dtype=np.int32)\n",
    "nf3_neg = Input(shape=(3,), dtype=np.int32)\n",
    "\n",
    "# Build model\n",
    "el_model = ELModel(nb_classes, nb_relations, embedding_size, batch_size, margin, reg_norm)\n",
    "out = el_model([nf1, nf2, nf3, nf4, dis, top, nf3_neg])\n",
    "model = tf.keras.Model(inputs=[nf1, nf2, nf3, nf4, dis, top, nf3_neg], outputs=out)\n",
    "optimizer = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Pandas files to store embeddings\n",
    "if not os.path.exists('data/elembeddings'):\n",
    "    os.makedirs('data/elembeddings')\n",
    "out_classes_file = f'data/elembeddings/{org_id}_cls_embeddings.pkl'\n",
    "out_relations_file = f'data/elembeddings/{org_id}_rel_embeddings.pkl'\n",
    "\n",
    "# ModelCheckpoint which runs at the end of each epoch\n",
    "checkpointer = MyModelCheckpoint(\n",
    "    out_classes_file=out_classes_file,\n",
    "    out_relations_file=out_relations_file,\n",
    "    cls_list=cls_list,\n",
    "    rel_list=rel_list,\n",
    "    valid_data=valid_data,\n",
    "    proteins=proteins,\n",
    "    monitor='loss')\n",
    "\n",
    "# Start training\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    workers=12,\n",
    "    callbacks=[checkpointer,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of embeddings on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 9606\n",
      "0.03 0.24 2442.07 0.85\n",
      "0.06 0.34 2371.22 0.86\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(data_file, classes, relations):\n",
    "    data = []\n",
    "    rel = f'<http://interacts>'\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = f'<http://{it[0]}>'\n",
    "            id2 = f'<http://{it[1]}>'\n",
    "            if id1 not in classes or id2 not in classes or rel not in relations:\n",
    "                continue\n",
    "            data.append((id1, rel, id2))\n",
    "    return data\n",
    "\n",
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "# Pandas files to store embeddings\n",
    "out_classes_file = f'data/elembeddings/{org_id}_cls_embeddings.pkl'\n",
    "out_relations_file = f'data/elembeddings/{org_id}_rel_embeddings.pkl'\n",
    "\n",
    "cls_df = pd.read_pickle(out_classes_file)\n",
    "rel_df = pd.read_pickle(out_relations_file)\n",
    "nb_classes = len(cls_df)\n",
    "nb_relations = len(rel_df)\n",
    "embeds_list = cls_df['embeddings'].values\n",
    "rembeds_list = rel_df['embeddings'].values\n",
    "size = len(embeds_list[0])\n",
    "embeds = np.zeros((nb_classes, size), dtype=np.float32)\n",
    "for i, emb in enumerate(embeds_list):\n",
    "    embeds[i, :] = emb\n",
    "\n",
    "rs = np.abs(embeds[:, -1]).reshape(-1, 1)\n",
    "embeds = embeds[:, :-1]\n",
    "prot_index = list(proteins.values())\n",
    "prot_rs = rs[prot_index, :]\n",
    "prot_embeds = embeds[prot_index, :]\n",
    "prot_dict = {v: k for k, v in enumerate(prot_index)}\n",
    "    \n",
    "rsize = len(rembeds_list[0])\n",
    "rembeds = np.zeros((nb_relations, rsize), dtype=np.float32)\n",
    "for i, emb in enumerate(rembeds_list):\n",
    "    rembeds[i, :] = emb\n",
    "\n",
    "train_data = load_test_data(f'data/train/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "valid_data = load_test_data(f'data/valid/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "trlabels = {}\n",
    "for c, r, d in train_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in trlabels:\n",
    "        trlabels[r] = np.ones((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    trlabels[r][c, d] = 1000\n",
    "for c, r, d in valid_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in trlabels:\n",
    "        trlabels[r] = np.ones((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    trlabels[r][c, d] = 1000\n",
    "\n",
    "test_data = load_test_data(f'data/test/{org_id}.protein.links.v11.0.txt', classes, relations)\n",
    "top1 = 0\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop1 = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "labels = {}\n",
    "preds = {}\n",
    "ranks = {}\n",
    "franks = {}\n",
    "eval_data = test_data\n",
    "n = len(eval_data)\n",
    "for c, r, d in eval_data:\n",
    "    c, r, d = prot_dict[classes[c]], relations[r], prot_dict[classes[d]]\n",
    "    if r not in labels:\n",
    "        labels[r] = np.zeros((len(prot_embeds), len(prot_embeds)), dtype=np.int32)\n",
    "    if r not in preds:\n",
    "        preds[r] = np.zeros((len(prot_embeds), len(prot_embeds)), dtype=np.float32)\n",
    "    labels[r][c, d] = 1\n",
    "    ec = prot_embeds[c, :]\n",
    "    rc = prot_rs[c, :]\n",
    "    er = rembeds[r, :]\n",
    "    ec += er\n",
    "\n",
    "    # Compute similarity\n",
    "    dst = np.linalg.norm(prot_embeds - ec.reshape(1, -1), axis=1)\n",
    "    dst = dst.reshape(-1, 1)\n",
    "    res = np.maximum(0, dst - rc - prot_rs - margin)\n",
    "    res = res.flatten()\n",
    "\n",
    "    preds[r][c, :] = res\n",
    "    index = rankdata(res, method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        top1 += 1\n",
    "    if rank <= 10:\n",
    "        top10 += 1\n",
    "    if rank <= 100:\n",
    "        top100 += 1\n",
    "    mean_rank += rank\n",
    "    if rank not in ranks:\n",
    "        ranks[rank] = 0\n",
    "    ranks[rank] += 1\n",
    "\n",
    "    # Filtered rank\n",
    "    index = rankdata((res * trlabels[r][c, :]), method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        ftop1 += 1\n",
    "    if rank <= 10:\n",
    "        ftop10 += 1\n",
    "    if rank <= 100:\n",
    "        ftop100 += 1\n",
    "    fmean_rank += rank\n",
    "\n",
    "    if rank not in franks:\n",
    "        franks[rank] = 0\n",
    "    franks[rank] += 1\n",
    "top1 /= n\n",
    "top10 /= n\n",
    "top100 /= n\n",
    "mean_rank /= n\n",
    "ftop1 /= n\n",
    "ftop10 /= n\n",
    "ftop100 /= n\n",
    "fmean_rank /= n\n",
    "\n",
    "rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "\n",
    "print(f'Evaluation for {org_id}')\n",
    "print(f'{top10:.2f} {top100:.2f} {mean_rank:.2f} {rank_auc:.2f}')\n",
    "print(f'{ftop10:.2f} {ftop100:.2f} {fmean_rank:.2f} {frank_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
